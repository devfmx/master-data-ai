{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent para dos Parámetros\n",
    "\n",
    "En este ejercicio vamos a implementar Gradient Descent para regresión linear. Aunque en este ejercicio sólo vamos a ir ajustando dos parámetros para encontrar la mejor línea (modelo) que represente la información.\n",
    "\n",
    "Vamos a utilizar la información de un archivo csv (comma separated values). En la siguiente celda importamos la información, y las separamos en trainX y trainY. En trainX, tenemos los features. En este caso sólo tenemos uno (asumamos que es el tamaño de la casa). En trainY tenemos el valor que querremos predecir, o sea el costo de la casa.\n",
    "\n",
    "En la siguiente celda graficamos estos puntos utilizando matplotlib en un scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "# Leemos la info de un archivo csv\n",
    "with open('data.csv', 'r') as f:\n",
    "  r = csv.reader(f)\n",
    "  points = list(r)\n",
    "\n",
    "# Separamos la información en trainX y trainY\n",
    "# En trainX tenemos nuestros features. \n",
    "# En este ejemplo sólo tenemos un feature, metros cuadrados\n",
    "# En trainY tenemos la variable que queremos predecir, en este caso\n",
    "# el valor de la casa en miles de dólares.\n",
    "trainX = []\n",
    "trainY = []\n",
    "for point in points:\n",
    "    trainX.append(float(point[0])) \n",
    "    trainY.append(float(point[1]))\n",
    "plt.scatter(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementaremos el algoritmo de Gradient Descent.\n",
    "\n",
    "Para comenzar, vamos a hacer una función que recibiendo el tamaño de la casa (x) y dos parámetros (theta1 y theta2), pueda calcular el precio. Esto utiliza una ecuación de una línea recta en 2 dimensiones (y = mx + b), pero recuerda que podemos tener muchos parámetros (y = a + bx + cx^2 + dx^3 + ex^4 + ... + fx^n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función h(x) = w*x + b Función que predice costo de casa\n",
    "def predict(size, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que predice el costo de una casa con una línea.\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    size: El tamaño de la casa en metros cuadrados\n",
    "    weight: El peso asignado al tamaño de la casa\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda deberás implementar la función de pérdida de Mean Squared Error.\n",
    "![MSE](https://github.com/TheHackerLlama/CursoDeepLearning/raw/42e5f3b542b78ae619101b1264e77b15c09364e3/Semana2/mse.png)\n",
    "\n",
    "Recuerda utilizar la función predict (h) para hacer más sencillo el algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función de pérdida y sus derivadas\n",
    "def loss_function(sizes, costs, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que calcula el error de una propuesta de regresión lineal\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    sizes: Los tamaños de las casas en metros cuadrados.\n",
    "    costs: Los costos verdaderos de las casas en cientos de miles de dólares.\n",
    "    weight: El peso asignado al tamaño de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    totalError = 0\n",
    "    m = len(sizes)\n",
    "    \n",
    "    # Calcula el error con diferencia de cuadrados\n",
    "    for i in range(0, m):\n",
    "        # ¿Cuál es el valor de entrada y cuál es el valor verdadero?\n",
    "        x = ...\n",
    "        y = ...\n",
    "        \n",
    "        # ¿Qué valor predicen tus datos? Utiliza la función predict\n",
    "        predicted = ...\n",
    "        \n",
    "        # Revisa la función para calcular el error\n",
    "        totalError += ...\n",
    "        \n",
    "    # Revisa la fórmula de MSE (Mean Squared Error)\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_deriv_weight(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el peso de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = (-1/n) * x * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "def loss_function_deriv_bias(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el bias de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = -1/n * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(sizes, costs, weight, bias, learning_rate):\n",
    "    \"\"\"\n",
    "    Función que propone un nuevo bias y peso que reduce la función de error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    weight: Los pesos de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    learning_rate: Qué tan rápido va a aprender\n",
    "    \"\"\"\n",
    "    \n",
    "    # El valor acumulado de la derivada en diferentes puntos. Inicialmente es 0.\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    \n",
    "    # El tamaño del dataset\n",
    "    m = float(len(sizes))\n",
    "    \n",
    "    # Para cada casa\n",
    "    for i in range(len(sizes)):\n",
    "        # ¿Cuál es su valor en X y en Y?\n",
    "        x = ...\n",
    "        y = ...\n",
    "        \n",
    "        # Calcula las derivadas parciales del loss function\n",
    "        bias_deriv += ...\n",
    "        weight_deriv += ...\n",
    "    \n",
    "    # Da un paso en la dirección que minimice el error\n",
    "    weight = weight - (weight_deriv * learning_rate)\n",
    "    bias = bias - (learning_rate * bias_deriv)\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "def train(sizes, costs, initial_weight, initial_bias, learning_rate, epoch):\n",
    "    \"\"\"\n",
    "    Función que utiliza Gradient Descent para minimizar error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    intial_weight: El peso inicial del modelo.\n",
    "    initial_bias: El bias inicial del modelo.\n",
    "    learning_rate: Qué tan rápido va a aprender.\n",
    "    epoch: Número de iteraciones sobre dataset.\n",
    "    \"\"\"\n",
    "    weight = initial_weight\n",
    "    bias = initial_bias\n",
    "   \n",
    "    # Itera varias veces sobre el dataset para minimizar el error\n",
    "    for i in range(epoch):\n",
    "        # Llama a step_gradient con sus argumentos correctos.\n",
    "        weight, bias = step_gradient(sizes, costs, weight, bias, learning_rate)\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"Loss: \", str(loss_function(sizes, costs, weight, bias)), \"b: \" , bias, \"w: \", weight)\n",
    "            error.append(loss_function(sizes, costs, weight, bias))\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "weight = 0.0\n",
    "bias = 0.0\n",
    "epochs = 100\n",
    "\n",
    "weight, bias = train(trainX, trainY, bias, weight, learning_rate, epochs)\n",
    "print(bias, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = 35\n",
    "predicted = predict(new_value, weight, bias)\n",
    "plt.scatter(trainX, trainY)\n",
    "plt.scatter(new_value, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(weight, bias):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(30, 70):\n",
    "        predicted = predict(i, weight, bias)\n",
    "        x.append(i)\n",
    "        y.append(predicted)\n",
    "    plt.scatter(trainX, trainY)\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "plot(weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(10), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_error = error[2:]\n",
    "plt.scatter(range(8), sm_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent para muchos parámetros (Reto)\n",
    "\n",
    "Esta sección es un reto. En esta parte, cargamos un dataset que tiene dos features. Es también un problema de regresión linear, pero, en este caso, es de muchos features. No copies directamente el código de arriba. El de arriba tenía el objetivo de ilustrar el algoritmo. Recuerda usar operaciones de matrices.\n",
    "\n",
    "Ejemplo:\n",
    "La hipótesis antes la calculamos con m*x + b.\n",
    "Ahora que tenemos muchos valores en x, podemos hacer theta' * transpuesta de X.\n",
    "\n",
    "Revisa la presentación 3 para revisar el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data2.csv', 'r') as f:\n",
    "  r = csv.reader(f)\n",
    "  points = list(r)\n",
    "points = np.array(points).astype(np.float)\n",
    "points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos X\n",
    "x = points[:, 0:2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos Y\n",
    "y = points[:, 2]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos columna de 1s a la matriz X\n",
    "intercept_term = np.ones(len(x))\n",
    "x = points[:, 0:2]\n",
    "\n",
    "# Agregamos la columna con los 0s\n",
    "X = np.c_[intercept_term, x]\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa una función train_multi que reciba una matriz X, \n",
    "# un vector y, una matriz theta, un learning_rate y el número de iteraciones.\n",
    "# Crea funciones adicionales si es necesario. Recuerda utilizar operaciones de matrices para tu ventaja.\n",
    "# Por ejemplo, la hipótesis la puedes calcular usando np.dot y np.transpose.\n",
    "def train_multi(X, y, theta, learning_rate, num_iters):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02;\n",
    "num_iters = 400;\n",
    "theta = np.zeros((3,1), dtype=float);\n",
    "\n",
    "theta = train_multi(X, y, theta, learning_rate, num_iters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuación normal\n",
    "La manera más corta de implementarlo, aunque no siempre es la mejor opción, es resolviéndolo analíticamente. Se puede hacer en pocas líneas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = np.dot(np.transpose(X),X)\n",
    "inverse = np.linalg.inv(first_term)\n",
    "transpose = np.transpose(X)\n",
    "\n",
    "result = np.dot(np.dot(inverse, transpose), y)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = X[0]\n",
    "testY = y[0]\n",
    "predict = np.dot(testX, result)\n",
    "print(\"Predicted: \" + str(predict) + \" Real: \" + str(testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función h(x) = w*x + b Función que predice costo de casa\n",
    "def predict(size, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que predice el costo de una casa con una línea.\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    size: El tamaño de la casa en metros cuadrados\n",
    "    weight: El peso asignado al tamaño de la casa\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    return size * weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementa la función de pérdida y sus derivadas\n",
    "def loss_function(sizes, costs, weight, bias):\n",
    "    \"\"\"\n",
    "    Función que calcula el error de una propuesta de regresión lineal\n",
    "    \n",
    "    Utiliza la función h(x) = x * w + b\n",
    "    \n",
    "    sizes: Los tamaños de las casas en metros cuadrados.\n",
    "    costs: Los costos verdaderos de las casas en cientos de miles de dólares.\n",
    "    weight: El peso asignado al tamaño de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    \"\"\"\n",
    "    totalError = 0\n",
    "    m = len(sizes)\n",
    "    \n",
    "    # Calcula el error con diferencia de cuadrados\n",
    "    for i in range(0, m):\n",
    "        # ¿Cuál es el valor de entrada y cuál es el valor verdadero?\n",
    "        x = sizes[i] \n",
    "        y = costs[i] \n",
    "        \n",
    "        # ¿Qué valor predicen tus datos? Utiliza la función predict\n",
    "        predicted = predict(x, weight, bias) \n",
    "        \n",
    "        # Revisa la función para calcular el error\n",
    "        totalError += (y - predicted) ** 2\n",
    "        \n",
    "    # Revisa la fórmula de MSE (Mean Squared Error)\n",
    "    return totalError / (2 * float(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_deriv_weight(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el peso de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = (-1/n) * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return -(1/n) * size * (cost - predict(size, weight, bias))\n",
    "\n",
    "def loss_function_deriv_bias(size, cost, weight, bias, n):\n",
    "    \"\"\"\n",
    "    Función que calcula la derivada parcial según el bias de MSE\n",
    "    \n",
    "    Utiliza la función h'(x) = -1/n * (y - (mx + b))\n",
    "    \n",
    "    size: El tamaño de una casa en metros cuadrados.\n",
    "    cost: El costo real de la casa.\n",
    "    weight: El peso asignado al tamaño de esa casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    n: El tamaño del dataset.\n",
    "    \"\"\"\n",
    "    return -(1/n) * (cost - predict(size, weight, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(sizes, costs, weight, bias, learning_rate):\n",
    "    \"\"\"\n",
    "    Función que propone un nuevo bias y peso que reduce la función de error.\n",
    "    \n",
    "    sizes: Los tamaños de la casa.\n",
    "    cost: Los costos reales de la casa.\n",
    "    weight: Los pesos de la casa.\n",
    "    bias: Para la regresión lineal, el valor predecido cuando el tamaño es 0.\n",
    "    learning_rate: Qué tan rápido va a aprender\n",
    "    \"\"\"\n",
    "    \n",
    "    # El valor acumulado de la derivada en diferentes puntos. Inicialmente es 0.\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    \n",
    "    # El tamaño del dataset\n",
    "    m = float(len(sizes))\n",
    "    \n",
    "    # Para cada casa\n",
    "    for i in range(len(sizes)):\n",
    "        # ¿Cuál es su valor en X y en Y?\n",
    "        x = sizes[i]\n",
    "        y = costs[i]\n",
    "        \n",
    "        # Calcula las derivadas parciales del loss function\n",
    "        bias_deriv += loss_function_deriv_bias(x, y, weight, bias, m)\n",
    "        weight_deriv += loss_function_deriv_weight(x, y, weight, bias, m)\n",
    "    \n",
    "    # Da un paso en la dirección que minimice el error\n",
    "    weight = weight - (weight_deriv * learning_rate)\n",
    "    bias = bias - (learning_rate * bias_deriv)\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips para el reto\n",
    "\n",
    "- Predicción: Calcula la hipótesis utilizando la transpuesta de X y usando np.dot con theta (o viceversa).\n",
    "\n",
    "- Derivada error (1): Resta la hipótesis menos la y real. Como ambos son matrices, sólo haz h - y\n",
    "\n",
    "- Derivada error (2): Recuerda multiplicar eso por X.\n",
    "\n",
    "- Derivada error (3): Multiplica eso por (1/m) y por alpha.\n",
    "\n",
    "- Derivada error (4): Resta theta - ese valor.\n",
    "\n",
    "- Itera sobre este proceso el número de iteraciones que es enviado como parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}